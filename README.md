**Named Entity Recognition (NER) with SimpleTransformers**
This project uses the SimpleTransformers library to implement a Named Entity Recognition (NER) model. The model is trained on a dataset and predicts named entities such as locations, organizations, and other relevant tags from text.

**Project Overview**
The objective of this project is to categorize named entities in text using the DistilBERT model, a smaller, faster variant of BERT. This approach leverages SimpleTransformers for quick setup and model training.

The NER model can predict named entities from input sentences, making it useful for various natural language processing tasks like extracting information from documents, social media posts, and more.

**Features**
Preprocessing of NER data: The dataset is processed and labeled for training and testing the NER model.
Training the model: The model is trained using DistilBERT to perform Named Entity Recognition (NER).
Evaluation and Prediction: The model is evaluated on test data, and it can predict named entities from a sample sentence.
**Technologies Used**
**Python**: Programming language used for data preprocessing and model training.
**SimpleTransformers**: A Python library for easy implementation of transformer-based models like BERT.
**DistilBERT**: A smaller, faster version of BERT for efficient training and inference.
**Scikit-learn**: Used for preprocessing data and splitting into training and testing datasets.
